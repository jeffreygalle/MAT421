{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO/+pvgLFAcXT7deY+4XOkx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffreygalle/MAT421/blob/main/ModuleD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Algebra"
      ],
      "metadata": {
        "id": "edBjI2V0XIoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Linear Spaces\n",
        "# 1.2.1.1 Linear Combinations:\n",
        "A linear combination in linear algebra is a new vector constructed from a subset by multiplying each vector by a constant and adding the results. A linear subspace is a result of linear combination.\n",
        "\n",
        "Linear subspace definiton: It is a space within a larger space that can be formed by combining vectors. It is a subset of a vector space that is closed under addition and scalar multiplication.\n",
        "\n",
        "Span defintion: Set of all possible vectors can be created by taking linear combinations of a given set of vectors within a vector space.\n",
        "\n",
        "Column space defintion: Span of the column vectors. It includes all possible linear combinations of the columns of the matrix.\n",
        "\n",
        "Column space Example:\n",
        "Given matrix A = \\begin{bmatrix} 1 & 3 \\\\ 2 & 6 \\end{bmatrix} the column vectors are v1 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} and v2 = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}.\n",
        "\n",
        "# 1.2.1.2 Linear Independence and Dimension\n",
        "Linear Independence defintion: A set of vectors is linearly independent if the only scalars that can produce a zero vector, when multiplied with the given vectors and then added together, are all zero. This means that no vector in the set is a linear combination of the others.\n",
        "\n",
        "Example:\n",
        "\n",
        "Two vectors in two-dimensional space, u = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} and v = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} \\.\n",
        "\n",
        "The span of these vectors is the set of all possible linear combinations of u and v. Any vector w in this space can be written as w = (a x u) + (b x v), where a and b are scalars.\n",
        "\n",
        "Basis of a space defintion: The basis of a vector space is the set of vectors that are linear independent and span the entire space.\n",
        "\n",
        "# 1.2.2 Orthogonality\n",
        "Norm defintion: Measures its length or magnitude of a vector. Denoted by ||v||. The norm is non-negative and equal to zero only if the vector is the zero vector.\n",
        "\n",
        "# 1.2.2.1 Orthonormal Bases\n",
        "Inner product defintion: Takes two vectors and produces a scalar. It measures the angle between two vectors. Denoted by <u, v>, where u and v are vectors.\n",
        "\n",
        "Orthonormal defintion: Each vector is both perpendicular to every other vector in the set and has a norm of 1. In other words, they are both perpendicular and normalized.\n",
        "\n",
        "Orthonormal example:\n",
        "\n",
        "The vectors u = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} and v = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} are orthonormal. They are perpendicular to each other and both have a length of 1.\n",
        "\n",
        "Orthogonal Projection defintion: The closest point on that line or subspace to the given point. Obtained by drawing a line perpendicular to the line or subspace and passing through the given point. The intersection of this perpendicular line with the line or subspace is the orthogonal projection point.\n",
        "\n",
        "Orthogonal Projection example:\n",
        "\n",
        "Given a point (4, 2) and a line L defined by the equation y = 2x. To find the orthogonal projection of P onto L, draw a perpendicular line from P to L. The point where the perpendicular line intersects L is the orthogonal projection point.The orthogonal projection point would be (2, 4).\n",
        "\n",
        "# 1.2.4 Eigenvalues and Eigenvectors\n",
        "\n",
        "Eigenvalues defintion: Represent scalar values by which a matrix can stretch or shrink a vector when multiplied by it.\n",
        "\n",
        "Eigenvalues Example:\n",
        "Given a 2x2 matrix A and a vector v. If Av is equal to a scalar multiple of v λv, then λ is an eigenvalue of matrix A and v is the corresponding eigenvector. The eigenvalue λ tells us the factor by which the vector v is scaled when multiplied by matrix A."
      ],
      "metadata": {
        "id": "uiMZyoJTXSmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Linear Regression\n",
        "# 1.3.1 QR Decomposition\n",
        "\n",
        "To obtain the QR decomposition or QR Factorization, we first use the Gram-Schmidt algorithm to orthogonalize a linearly independent set of vectors. The Gram-Schmidt algorithm  constructs an orthonormal basis from the given set of vectors. The basis vectors are perpendicular to each other and have a length of 1.\n",
        "\n",
        "\n",
        "# 1.3.2 Least-squares Problem defintion: Method to find the best-fitting solution to an over-determined system (more equations than unknowns (n > m).) Goal: minimize the sum of the squares of the residuals, which are the differences between the observed values and the values predicted by the model.\n",
        "\n",
        "Goal part-2: minimize the objective function ||Ax - b||^2, where ||.|| denotes the Euclidean norm. The solution to this minimization problem is given by the equation AᵀAx = Aᵀb, assuming A has linearly independent columns. The matrix AᵀA is guaranteed to be invertible, and the solution can be found as:\n",
        "x = (AᵀA)⁻¹Aᵀb.\n",
        "\n",
        "\n",
        "# 1.3.3 Linear Regression\n",
        "\n",
        "In linear regression the goal is to find an affine function (takes an input, multiplies it by a constant, adds another constant to it, and produces an output) that best fits a given set of input data points {(xi, yi)}. The input data points consist of a set of xi values, where each xi is a d-dimensional vector (xi1, ..., xid)T, and their corresponding yi values. The affine function has the form f(xi) = β0 + β1xi1 + ... + βd xid, where β0, β1, ..., βd are the coefficients of the function.\n",
        "\n",
        "How: use Least Squares. Least squares minimizes the sum of the squared differences between the predicted values and the actual values of the yi. The coefficients β0, β1, ..., βd are estimated to minimize this sum of squared differences, giving us the best-fitting affine function.\n",
        "\n",
        "Once the coefficients are found, use the linear regression model to make predictions for new input data points"
      ],
      "metadata": {
        "id": "7p7QYo0e8mUq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I16-NF20ZeZQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}